{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages for data manupulation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "# LIME SECTION\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from __future__ import print_function\n",
    "plt.show()\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xavient/avidya'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUT: set path for data placement \n",
    "#os.chdir(r'C:\\Users\\mshafin\\Desktop\\xavient_churn_project')\n",
    "#os.chdir(r'C:\\Users\\mohammad shafin\\Desktop\\xavient_churn_project')\n",
    "#os.chdir(r'/home/devbox2/income')\n",
    "#os.chdir(r'/home/devbox2/xavient_churn_project')\n",
    "#os.chdir(r'/home/devbox2/bankloan')\n",
    "#os.chdir(r'/home/devbox2/creditcardfraud')\n",
    "#os.chdir(r'/home/xavient/santander')\n",
    "#os.chdir(r'/home/xavient/homecredit')\n",
    "os.chdir(r'/home/xavient/avidya')\n",
    "#os.chdir(r'/home/xavient/income')\n",
    "#os.chdir(r'/home/xavient/xavient_churn_project')\n",
    "#os.chdir(r'/home/xavient/bankloan')\n",
    "#os.chdir(r'/home/xavient/creditcardfraud')\n",
    "random.seed(42)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: data loaded to a pandas dataframe\n",
    "#df = pd.read_csv('validation.csv')\n",
    "#df = pd.read_csv('application_test.csv')\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "df_length = df.shape[1]\n",
    "df_row = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in_num = open(\"no_of_features.pickle\",\"rb\")\n",
    "no_of_features = pickle.load(pickle_in_num)\n",
    "no_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.poutcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.month.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.info())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of missing values in each column\n",
    "#print(\"Columnwise missing value count\")\n",
    "#view = df.isnull().sum().plot.bar(figsize=(df_length, 4))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in_num = open(\"customer_identity_code.pickle\",\"rb\")\n",
    "customer_identity_code = pickle.load(pickle_in_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in_num = open(\"target_code.pickle\",\"rb\")\n",
    "target_code = pickle.load(pickle_in_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in_num = open(\"label_target1.pickle\",\"rb\")\n",
    "label_target1 = pickle.load(pickle_in_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in_num = open(\"label_target0.pickle\",\"rb\")\n",
    "label_target0 = pickle.load(pickle_in_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in_num = open(\"numeric_columns_mean_final_nz.pickle\",\"rb\")\n",
    "numeric_columns_mean_final_nz_index = pickle.load(pickle_in_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in_num = open(\"numeric_columns_mean_final_nz.pickle\",\"rb\")\n",
    "numeric_columns_mean_final_nz_index = pickle.load(pickle_in_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index for customer identification code\n",
    "\n",
    "df = df.set_index(customer_identity_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify target variable\n",
    "\n",
    "#df['target'] = df[target_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to recode target levels\n",
    "\n",
    "def dependent_col(row):\n",
    "    if row['target'] == label_target1:\n",
    "        val = 1  # input\n",
    "    elif row['target'] == label_target0:\n",
    "        val = 0  # imput\n",
    "    else:\n",
    "        val = 2\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view recoded data in the target variable\n",
    "#df['target'] = df.apply(dependent_col, axis=1)\n",
    "#print(df.loc[:,['target','Churn']].head(5)) #check changes in target recoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep rows when target variable is finite\n",
    "\n",
    "#start = df.shape[1]\n",
    "#df = df[np.isfinite(df['target'])]\n",
    "#finish = df.shape[1]\n",
    "#print(\"The number of row/rows dropped because of missing target variable is \" + str(start-finish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target variable distibution\n",
    "\n",
    "#df[\"target\"].value_counts().plot.pie(figsize=(4, 4))\n",
    "#print(df.target.value_counts())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load numeric columns for scoring\n",
    "\n",
    "pickle_in_cat = open(\"object_columns_final_nz.pickle\",\"rb\")\n",
    "object_columns_final_nz = pickle.load(pickle_in_cat)\n",
    "#print(\"no of columns in categorical type \" + str(len(object_columns_final_nz_index)))\n",
    "len(object_columns_final_nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value imputation strategy: create a new level called 'UNKNOWN'\n",
    "\n",
    "for column in object_columns_final_nz:\n",
    "    if df[column].dtypes==\"object\":\n",
    "        df[column] = df[column].fillna(\"UKNOWN\").astype('object')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numeric columns for scoring\n",
    "\n",
    "pickle_in_num = open(\"numeric_columns_mean_final_nz.pickle\",\"rb\")\n",
    "numeric_columns_mean_final_nz_index = pickle.load(pickle_in_num)\n",
    "\n",
    "pickle_in_num = open(\"numeric_columns_zero_final_nz.pickle\",\"rb\")\n",
    "numeric_columns_zero_final_nz_index = pickle.load(pickle_in_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mean imputation\n",
    "if numeric_columns_mean_final_nz_index == list():\n",
    "    pass\n",
    "else:\n",
    "    for column in numeric_columns_mean_final_nz_index:\n",
    "        if df[column].dtypes in [\"int64\",\"float64\"] :\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "\n",
    "# for 0 imputation\n",
    "if numeric_columns_zero_final_nz_index == list():\n",
    "    pass\n",
    "else:\n",
    "    for column in numeric_columns_zero_final_nz_index:\n",
    "        if df[column].dtypes in [\"int64\",\"float64\"] :\n",
    "            df[column] = df[column].fillna(0)\n",
    "        \n",
    "scale_columns_final_nz = numeric_columns_mean_final_nz_index + numeric_columns_zero_final_nz_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if scale_columns_final_nz == list():\n",
    "#    pass\n",
    "#else:\n",
    "#    df[scale_columns_final_nz].plot.box(figsize=(len(scale_columns_final_nz)*2,4))\n",
    "#    print(\"Check for continous variable scaling\")\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the numerical variables to bring the numerical variables to the same scale for fault tolerance\n",
    "from sklearn.preprocessing import scale\n",
    "if scale_columns_final_nz == list():\n",
    "    pass\n",
    "else:\n",
    "    for column in scale_columns_final_nz:\n",
    "        if df[column].dtypes in [\"int64\",\"float64\"] :\n",
    "            df[column] = scale(df[column].astype('float64'))\n",
    "    #print(\"scaled numeric_columns_mean variables\")\n",
    "    #print(df[scale_columns_final_nz].columns)\n",
    "    #df[scale_columns_final_nz].plot.box(figsize=(len(scale_columns_final_nz)*2,4))\n",
    "    #print(\"number of numeric columns: \"+ str(len(scale_columns_final_nz)))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_in_num = open(\"final_column_list.pickle\",\"rb\")\n",
    "final_column_list = pickle.load(pickle_in_num)\n",
    "len(final_column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns_final_nz = intersection(final_column_list,scale_columns_final_nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns_final_nz = intersection(final_column_list,object_columns_final_nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_in_num = open(\"final_feature_selection.pickle\",\"rb\")\n",
    "final_feature_selection = pickle.load(pickle_in_num)\n",
    "len(final_feature_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns_final_nz = intersection(final_feature_selection,scale_columns_final_nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns_final_nz = intersection(final_feature_selection,object_columns_final_nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "target_column = ['target']\n",
    "float_columns = df[scale_columns_final_nz].columns\n",
    "cat_columns = df[object_columns_final_nz].columns\n",
    "final = pd.concat([df[object_columns_final_nz],df[scale_columns_final_nz]],axis=1)\n",
    "# Extract data \n",
    "train_float_features = df[float_columns]\n",
    "train_cat_features = df[cat_columns]\n",
    "\n",
    "#train_int_features = train_df[int_columns]\n",
    "#print(train_cat_features.shape)\n",
    "#print(train_float_features.shape)\n",
    "\n",
    "#print(float_columns)\n",
    "#print(cat_columns)\n",
    "#print(final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Label Encoding:\n",
    "#train_cat_features_ver2 = pd.get_dummies(train_cat_features, columns=['Destination_Type','Type_of_Cab'])\n",
    "train_cat_features_ver2 = train_cat_features.apply(LabelEncoder().fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34224, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cat_features_ver2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns_final_nz_2 = df[scale_columns_final_nz]\n",
    "#### Finalize X & Y\n",
    "if list(train_cat_features_ver2) == []:\n",
    "    train_transformed_features =  scale_columns_final_nz_2\n",
    "else:\n",
    "    train_transformed_features = np.concatenate((train_cat_features_ver2,scale_columns_final_nz_2),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_transformed_features = np.concatenate((temp_1,train_int_features),axis=1)\n",
    "train_transformed_features = pd.DataFrame(data=train_transformed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.631539</td>\n",
       "      <td>-0.561222</td>\n",
       "      <td>-0.347754</td>\n",
       "      <td>-0.177465</td>\n",
       "      <td>-0.189173</td>\n",
       "      <td>0.243714</td>\n",
       "      <td>-0.745751</td>\n",
       "      <td>-0.813427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.936021</td>\n",
       "      <td>0.304607</td>\n",
       "      <td>-0.347754</td>\n",
       "      <td>-0.177465</td>\n",
       "      <td>-0.189173</td>\n",
       "      <td>0.193481</td>\n",
       "      <td>1.554513</td>\n",
       "      <td>0.093413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.315302</td>\n",
       "      <td>-0.062128</td>\n",
       "      <td>-0.347754</td>\n",
       "      <td>-0.177465</td>\n",
       "      <td>-0.189173</td>\n",
       "      <td>-4.983207</td>\n",
       "      <td>-1.704195</td>\n",
       "      <td>0.093413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.525349</td>\n",
       "      <td>-0.293359</td>\n",
       "      <td>-0.347754</td>\n",
       "      <td>-0.177465</td>\n",
       "      <td>-0.189173</td>\n",
       "      <td>0.197144</td>\n",
       "      <td>0.021004</td>\n",
       "      <td>-0.586717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.569293</td>\n",
       "      <td>-0.010361</td>\n",
       "      <td>1.110230</td>\n",
       "      <td>-0.177465</td>\n",
       "      <td>-0.189173</td>\n",
       "      <td>0.206039</td>\n",
       "      <td>0.596070</td>\n",
       "      <td>-0.133297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6         7  \\\n",
       "0  0.0  1.631539 -0.561222 -0.347754 -0.177465 -0.189173  0.243714 -0.745751   \n",
       "1  0.0  0.936021  0.304607 -0.347754 -0.177465 -0.189173  0.193481  1.554513   \n",
       "2  1.0 -0.315302 -0.062128 -0.347754 -0.177465 -0.189173 -4.983207 -1.704195   \n",
       "3  1.0 -0.525349 -0.293359 -0.347754 -0.177465 -0.189173  0.197144  0.021004   \n",
       "4  0.0 -1.569293 -0.010361  1.110230 -0.177465 -0.189173  0.206039  0.596070   \n",
       "\n",
       "          8  \n",
       "0 -0.813427  \n",
       "1  0.093413  \n",
       "2  0.093413  \n",
       "3 -0.586717  \n",
       "4 -0.133297  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transformed_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabels = df[target_column]#.values.ravel()\\nle= sklearn.preprocessing.LabelEncoder()\\nle.fit(labels)\\nlabels = le.transform(labels)\\nclass_names = list(le.classes_)\\ntarget_names = np.array(class_names, dtype=str)\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "labels = df[target_column]#.values.ravel()\n",
    "le= sklearn.preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels = le.transform(labels)\n",
    "class_names = list(le.classes_)\n",
    "target_names = np.array(class_names, dtype=str)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = train_transformed_features.values\n",
    "number_of_features = len(array[0])\n",
    "X = array[:,0:number_of_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34224, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xavient/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import metrics for model evaluation\n",
    "\n",
    "from sklearn.metrics import recall_score,accuracy_score,confusion_matrix,classification_report,precision_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# create function to evaluate model performance\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred, sample_weight=None)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(\"accuracy :\" +str(accuracy))\n",
    "    print(\"cohen_kappa :\" +str(cohen_kappa))\n",
    "    print(\"recall :\" +str(recall))\n",
    "    print(\" tn, fp, fn, tp :\" )\n",
    "    print(tn, fp, fn, tp)\n",
    "    print(\"matrix :\")\n",
    "    print(matrix)\n",
    "    print(\"report :\")\n",
    "    print(report)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf loaded: \n",
      "ada loaded: \n",
      "xgb loaded: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xavient/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/xavient/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "import pickle\n",
    "############################# SMOTE\n",
    "# load the rf from disk\n",
    "rf = pickle.load(open('finalized_rf.sav', 'rb'))\n",
    "#result = rf.score(X, labels)\n",
    "print(\"rf loaded: \")# + str(result))\n",
    "# load the adaboost from disk\n",
    "#ada = pickle.load(open('finalized_ada.sav', 'rb'))\n",
    "#result = ada_smote.score(X, labels)\n",
    "print(\"ada loaded: \")# + str(result))\n",
    "# load the gbm from disk\n",
    "gs = pickle.load(open('finalized_xgb.sav', 'rb'))\n",
    "#result = gs_smote.score(X, labels)\n",
    "print(\"xgb loaded: \")# + str(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'X_train_res.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-56a2debe5696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle_in_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train_res.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_in_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'X_train_res.pickle'"
     ]
    }
   ],
   "source": [
    "pickle_in_num = open(\"X_train_res.pickle\",\"rb\")\n",
    "X_train_res = pickle.load(pickle_in_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3bb4f9c28673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# load weights into new model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_json.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded model from disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1180\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m    927\u001b[0m                              ' elements.')\n\u001b[1;32m    928\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2433\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2435\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n\u001b[1;32m    182\u001b[0m                                         allow_soft_placement=True)\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0m_SESSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \"\"\"\n\u001b[0;32m-> 1509\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    636\u001b[0m           \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewDeprecatedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteSessionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "# load json and create model\n",
    "json_file = open('model_json.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('model_json.h5')\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#This is a multiline comment\n",
    "\n",
    "def evaluate_accuracy(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)*100\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_recall(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    recall = recall_score(y_test,y_pred)*100\n",
    "    return recall\n",
    "\n",
    "def evaluate_precision(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    precision = precision_score(y_test,y_pred)*100\n",
    "    return precision\n",
    "\n",
    "def evaluate_con_mat_row(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    return (tn, fp, fn, tp)\n",
    "\n",
    "################ keras model\n",
    "def evaluate_accuracy_keras(model, X_test, y_test):\n",
    "    y_pred = loaded_model.predict_classes(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)*100\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_recall_keras(model, X_test, y_test):\n",
    "    y_pred = loaded_model.predict_classes(X_test)\n",
    "    recall = recall_score(y_test,y_pred)*100\n",
    "    return recall\n",
    "\n",
    "def evaluate_precision_keras(model, X_test, y_test):\n",
    "    y_pred = loaded_model.predict_classes(X_test)\n",
    "    precision = precision_score(y_test,y_pred)*100\n",
    "    return precision\n",
    "\n",
    "def evaluate_con_mat_row_keras(model, X_test, y_test):\n",
    "    y_pred = loaded_model.predict_classes(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    return (tn, fp, fn, tp) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "xgbm_accuracy = evaluate_accuracy(gs,X,labels)\n",
    "rf_accuracy = evaluate_accuracy(rf,X,labels)\n",
    "ada_accuracy = evaluate_accuracy(ada,X,labels)\n",
    "tf_accuracy = evaluate_accuracy_keras(loaded_model,X,labels)\n",
    "\n",
    "xgbm_recall = evaluate_recall(gs,X,labels)\n",
    "rf_recall = evaluate_recall(rf,X,labels)\n",
    "ada_recall = evaluate_recall(ada,X,labels)\n",
    "tf_recall = evaluate_recall_keras(loaded_model,X,labels)\n",
    "\n",
    "xgbm_precision = evaluate_precision(gs,X,labels)\n",
    "rf_precision = evaluate_precision(rf,X,labels)\n",
    "ada_precision = evaluate_precision(ada,X,labels)\n",
    "tf_precision = evaluate_precision_keras(loaded_model,X,labels)\n",
    "\n",
    "xgbm_tfft = evaluate_con_mat_row(gs,X,labels)\n",
    "rf_tfft = evaluate_con_mat_row(rf,X,labels)\n",
    "ada_tfft = evaluate_con_mat_row(ada,X,labels)\n",
    "tf_tfft = evaluate_con_mat_row_keras(loaded_model,X,labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "report = [{'model': 'XGBM', 'accuracy%': xgbm_accuracy, 'hit rate%': xgbm_recall, 'precision%': xgbm_precision,'tn, fp, fn, tp': xgbm_tfft},\n",
    "          {'model': 'RF',  'accuracy%': rf_accuracy, 'hit rate%': rf_recall, 'precision%': rf_precision,'tn, fp, fn, tp': rf_tfft},\n",
    "          {'model': 'ADA', 'accuracy%': ada_accuracy, 'hit rate%': ada_recall, 'precision%': ada_precision ,'tn, fp, fn, tp': ada_tfft },\n",
    "          {'model': 'tf', 'accuracy%': tf_accuracy, 'hit rate%': tf_recall, 'precision%': tf_precision ,'tn, fp, fn, tp': tf_tfft }]\n",
    "df1 = pd.DataFrame(report)\n",
    "df1 = df1[['model', 'accuracy%', 'hit rate%', 'precision%','tn, fp, fn, tp']]\n",
    "df1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.plot.bar(figsize=(df_length, 4))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME SECTION\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from __future__ import print_function\n",
    "\n",
    "predict_fn_rf = lambda x: rf.best_estimator_.predict_proba(x).astype(float)\n",
    "predict_fn_ada = lambda x: ada.best_estimator_.predict_proba(x).astype(float)\n",
    "predict_fn_xgb = lambda x: gs.best_estimator_.predict_proba(x).astype(float)\n",
    "predict_fn_tf = lambda x: np.hstack((1-loaded_model.predict_proba(x),loaded_model.predict_proba(x))).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'feature_names.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-80e8d770a03d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle_in_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feature_names.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_in_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpickle_in_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"target_names.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_in_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'feature_names.pickle'"
     ]
    }
   ],
   "source": [
    "pickle_in_num = open(\"feature_names.pickle\",\"rb\")\n",
    "feature_names = pickle.load(pickle_in_num)\n",
    "\n",
    "pickle_in_num = open(\"target_names.pickle\",\"rb\")\n",
    "target_names = pickle.load(pickle_in_num)\n",
    "\n",
    "pickle_in_num = open(\"cat_columns.pickle\",\"rb\")\n",
    "cat_columns = pickle.load(pickle_in_num)\n",
    "\n",
    "pickle_in_num = open(\"feature_names_cat.pickle\",\"rb\")\n",
    "feature_names_cat = pickle.load(pickle_in_num)\n",
    "\n",
    "#print(\"feature_names :\" + str(feature_names))\n",
    "#print(\"target_names :\" + str(target_names))\n",
    "#print(\"cat_columns :\" + str(cat_columns))\n",
    "#print(\"feature_names_cat :\" + str(feature_names_cat))\n",
    "#print(\"X_train_res.shape :\" + str(X_train_res.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-b31e8333e3b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the LIME Explainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m explainer = lime.lime_tabular.LimeTabularExplainer(X_train_res ,feature_names = feature_names,class_names=target_names,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                    \u001b[0mcategorical_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                    categorical_names = feature_names_cat, kernel_width = 3, discretize_continuous = True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_res' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the LIME Explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_res ,feature_names = feature_names,class_names=target_names,\n",
    "                                                   categorical_features = cat_columns, \n",
    "                                                   categorical_names = feature_names_cat, kernel_width = 3, discretize_continuous = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the observation in the validation set for which explanation is required\n",
    "observation_1 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8c5552cdcea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the explanation for RF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_fn_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_in_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'explainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the explanation for RF\n",
    "exp = explainer.explain_instance(X[observation_1], predict_fn_tf, num_features=5)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels[observation_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = gs.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gs.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pred).to_csv(\"pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
